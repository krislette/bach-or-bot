mlp:
  hidden_layers: [512, 256, 128] # 3 hidden layers
  dropout: [0.5, 0.4, 0.3] # Dropout rates for each layer
  learning_rate: 0.0001 # Adam optimizer
  batch_size: 128 # Number of samples processed together
  epochs: 200 # Maximum training iterations
  patience: 15 # Early stopping patience

  weight_decay: 0.01 # L2 regularization
  gradient_clipping: 0.5 # Prevent exploding gradients
  mixup_alpha: 0.1 # For data augmentation during training, 0 disables MixUp
