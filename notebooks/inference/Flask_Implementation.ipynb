{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4iI-Ak9YOBcf",
    "outputId": "68d0b2a3-7361-4c4a-f548-a606348b14d9"
   },
   "outputs": [],
   "source": [
    "pip install flask pyngrok flask_ngrok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v65upGIEDiyo",
    "outputId": "c5733572-e03e-40f3-c4a5-a72445089f36"
   },
   "outputs": [],
   "source": [
    "pip install  torch llm2vec librosa pandas soundfile torchaudio peft timm pyyaml torchao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669,
     "referenced_widgets": [
      "92bb6838411b4c7e905877ba7950e59e",
      "f66950e860c14337a9718d31de270d7e",
      "5693aef6b0d947deb41e3bbc216222b5",
      "bbf42610db674d9392d3a8975defb01d",
      "439de93625664643ae90f018eda184af",
      "9355e67c821342a9878bd94cbf50173b",
      "44be670b9cd74be0bd99d5e1e7506b22",
      "234cca1fa292449c9dad413ea2944f72",
      "d00d9cc691f94bfb8985e56824a8ee6f",
      "842eaff65fd84fb183ebde296106e078",
      "08ed66f08def43f4a32d09bf4e4028c9",
      "e9a2ad69cc0d4fb2bfcf8a7c0fb6b617",
      "5fe33d8f1e0941b7aaec2713fa573135",
      "7ad8a40e72614de691ca1e78f3f07d12",
      "784b0db881af45378a4064e572001194",
      "c75a7e7973694b5f94eaec54c909cdb1",
      "60aa231bf81b41b7a71cb522f2b9b092",
      "77ad25e4692e463caf508db8ef94eb70",
      "6b1ac2a619a54307bd22fa6506033abd",
      "26918f66b1474787a78bb6c2c3c5433a",
      "58b70fbd75974282af6bc89f733ccac8",
      "f5f5e5fd3bb74ef384a308af38297620",
      "1ecec752c85343d0818fe0ac62bb89bd",
      "08d297c0d69e4b29b0a48dff74c6d026",
      "d33b0905870c4ce8868ad4493157e763",
      "a48096d242854880ab07dec72fb13367",
      "2fa4648707014131a57f814a78704a18",
      "2ecab22e74c54f85a408ca6d4af813fb",
      "343b96b814894eea8c717b97ff2aa0d9",
      "d23e9d77079343aeb1d798d3274933f5",
      "75696690df3a42908e15328e77a3e247",
      "6716ffaeae5d4d84a70642840b41ad4a",
      "21e0a9ffab914e09a75e4f475f8840fe",
      "f413a0478e7d40578833d154c39795cd",
      "5687fc7d817d44a282ac591ad191aaa1",
      "5e7b68ffb26b40b28db17a63140cc680",
      "4a5988c6511c470681b3f2e97d9d6061",
      "edbff2efea68458194b817be7be3a048",
      "b3e6abeb06994693bc86bf042c264342",
      "b6ef4e8351e14a96967e5862498d6fb7",
      "f8362e4d54bd402e8bbc9cf7407ac00d",
      "3ec7057685704025bae0f88aa73f8a01",
      "4e1eefc5afb841cd9534d3d7c807bcca",
      "061dca0a0f694a95964b4b43a6ada4c8",
      "6ee41d7e748541059b0f1f8ba676edeb",
      "338140d34c894ababd0a2af60b2e95cb",
      "15229adc1d5d4636bd22f43a3a40e18c",
      "3ae8d6e50b8e47c7ad3c779127069ae0",
      "efa9d781642947af9926629bde1812c4",
      "1fdc2f5bb4e040cf8362263a3dfb6233",
      "30bd4acec5eb48c9860109a0cbba7b56",
      "78723c9a698d4ecd995f160a8cc32467",
      "3613e9f77912403b97bf7522012caa30",
      "7e1334d2dd964db58575f7977b619c42",
      "fa272c85996a470aabf1a694b827916c",
      "a9a9a476e1364b57aa8c7fcfed2cb069",
      "660278485abe4e269528c6d25ce5fe13",
      "2a94c8158e3c450aaac695a354c9a13e",
      "b26bca41ed4741139884a84a1d62740b",
      "980e07d42c8c45fe9645cb3e5157ca86",
      "f6cc351a0a0442e3a143f4f1dbe6cb1a",
      "889ebd886d084596b610b73687866adb",
      "531d7b33e7cc4c049974e6988e666bad",
      "d45d4d1e04ee47fbb846cbd9da8fd641",
      "af7c30248ac144f0b8118a977f94bb72",
      "bde567f385854f119fcc5f10991f8ec7",
      "06f7aa83713c4fc3ae3f4b87b0c7cbe1",
      "d721a36fa6ab4a939d1788c4507ee3e3",
      "3da351d5a10c4c8f8d5ca4a5e4a8a7c4",
      "52399f3667df423ea1f62c64f254e4b8",
      "b42cc3d3f4054ac29b49a694d59abadb",
      "634e3da4620f4aaf9d01830c1baca341",
      "82b97619d1fc4fec98578e732b9f1eec",
      "0ce5f9dc36264bb3beff1889cd5928c7",
      "af40d17fd0d44ad89eeedbfde7723b98",
      "1a527a9988d14ce1b1bd17e3be59be0e",
      "a16615216de1455b809c1e768d191517",
      "806648f422154e0590fa85680290e6e4",
      "8107e2789a4c44c18011e2b0fbf9914e",
      "f0d0c403d6ed4b31a6c757ffe77f831d",
      "a49adb6d2d014914bf2f1f5828c0a092",
      "d8ea708640504662bd7b553d539f643b",
      "dac63dc2594d4f769806ebee33795b00",
      "5c4e41886c3348a5b11840eb20bba3ac",
      "c77a708ea7b3431ab092864fbfa62784",
      "0a8b736760dd4a5dab807ded28ba455b",
      "9017f80b91dd4b899786866a42566f7d",
      "25b2f429f4e244838f54fbded3eeedb8",
      "133bdc3ce29845709a12f0af3a07bbfa",
      "0127a2fafb5b44a1a4d347f0dfb45114",
      "c97a11066518486aa9829bb3b1cdecc2",
      "ed8d8e9dbcda415a8dfe7af5a5f8d970",
      "86f5a1fe899f44d68ba88e9195dc5066",
      "c6c2d7e6c7de4b328c5a94536f6bae50",
      "2aa6180761984798b51ffba386ce3606",
      "844b79eee7a1410f92a2b7d231b767ef",
      "bbbfe44202864353b5aaf4441e08bbee",
      "4f6eec517e9c4e248bb6f50dc3eba44f",
      "6428de79af294d35a04f755041a751ae",
      "d69f116dad664847a61ed926fd95e389",
      "ebed635b896a417f977d4638a52919f6",
      "284bed26456e4e55901c423e9118b64f",
      "9e43d2ea81ea4a6b89a851ac6987774f",
      "7e696d71e303431fbdf2a97f56e57c4a",
      "e3ab96f5874c41ba9d2ce693a4d61087",
      "4514632832cc4b92a763986ebb950a1a",
      "046e5b40a7aa442b800422708cee60a1",
      "764af49b76bd40eaa52a078c9c8b7255",
      "cd02854344f44b90b5de1a645971c815",
      "afbc3a443ffd44e48442d9f6025715ba"
     ]
    },
    "id": "qPUI-T33AIHN",
    "outputId": "6cbd2d7b-7231-4b5d-f808-df742e9b8041"
   },
   "outputs": [],
   "source": [
    "from llm2vec import LLM2Vec\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from peft import PeftModel\n",
    "from torchao.quantization import quantize_, Int8WeightOnlyConfig\n",
    "import torch\n",
    "\n",
    "access_token = REDACTED_TOKEN\n",
    "\n",
    "model_id = \"McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding = True, truncation = True, max_length = 512)\n",
    "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU\")\n",
    "# GPU path: use bf16 for speed\n",
    "    model = AutoModel.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\",\n",
    "    token=access_token,\n",
    ")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "# CPU path: use float32 first, then quantize\n",
    "    model = AutoModel.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=config,\n",
    "    torch_dtype=torch.float32,   # quantization requires fp32\n",
    "    device_map=\"cpu\",\n",
    "    token=access_token,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        from torchao.quantization import quantize_\n",
    "        print(\"[INFO] Applying torchao quantization for CPU...\")\n",
    "        quant_config = Int8WeightOnlyConfig(group_size=None)\n",
    "        print(\"[INFO] Applying torchao quantization with Int8WeightOnlyConfig...\")\n",
    "        quantize_(model, quant_config)\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"[WARNING] torchao not installed. Run: pip install torchao\")\n",
    "        print(\"[WARNING] Falling back to non-quantized CPU model.\")\n",
    "\n",
    "l2v = LLM2Vec(model, tokenizer, pooling_mode=\"mean\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKN2lEaxDyng"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Initialize PCA and StandardScaler globally for training\n",
    "_pca_trainer = None\n",
    "\n",
    "class SimplePCATrainer:\n",
    "    \"\"\"\n",
    "    A simple PCA trainer that uses IncrementalPCA to fit data in batches.\n",
    "    It saves checkpoints every 5 batches and can save the final model.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Attributes:\n",
    "        pca: The IncrementalPCA model.\n",
    "        scaler: StandardScaler for normalizing data.\n",
    "        fitted: Boolean indicating if the model has been initialized.\n",
    "        batch_count_pca: Counter for the number of batches processed.\n",
    "\n",
    "    Methods:\n",
    "        process_batch(vectors): Processes a batch of vectors, fits the PCA model incrementally.\n",
    "        save_final(model_path): Saves the final PCA model to the specified path.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the trainer\n",
    "    def __init__(self):\n",
    "        self.pca = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.fitted = False\n",
    "        self.batch_count_pca = 0\n",
    "\n",
    "    def _determine_optimal_components(self, vectors):\n",
    "        \"\"\"\n",
    "        Determine the optimal number of PCA components to retain 95% variance.\n",
    "\n",
    "        Args:\n",
    "            vectors: The input data to analyze.\n",
    "        Returns:\n",
    "            n_components: The optimal number of components.\n",
    "        \"\"\"\n",
    "        temp_pca = IncrementalPCA()\n",
    "        temp_pca.fit(vectors)\n",
    "        cumsum_var = np.cumsum(temp_pca.explained_variance_ratio_)\n",
    "        n_comp_95 = np.argmax(cumsum_var >= 0.95) + 1\n",
    "        return min(n_comp_95, vectors.shape[1] // 2)\n",
    "\n",
    "    def process_batch(self, vectors):\n",
    "        \"\"\"\n",
    "        Process a batch of vectors, fitting the PCA model incrementally.\n",
    "\n",
    "        Args:\n",
    "            vectors: The input data batch to process.\n",
    "        Returns:\n",
    "            reduced_vectors: The PCA-transformed data.\n",
    "\n",
    "        Note: This method saves a checkpoint every 5 batches.\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            # First batch - initialize everything\n",
    "            n_components = self._determine_optimal_components(vectors)\n",
    "            self.pca = IncrementalPCA(n_components=n_components, batch_size=1000)\n",
    "            self.scaler.fit(vectors)\n",
    "            self.fitted = True\n",
    "            print(f\"Initialized PCA with {n_components} components\")\n",
    "\n",
    "        # Process batch\n",
    "        vectors_scaled = self.scaler.transform(vectors)\n",
    "        self.pca.partial_fit(vectors_scaled)\n",
    "        reduced_vectors = self.pca.transform(vectors_scaled)\n",
    "\n",
    "        self.batch_count_pca += 1\n",
    "\n",
    "        # Save checkpoint every 5 batches\n",
    "        if self.batch_count_pca % 5 == 0:\n",
    "            os.makedirs(\"pca_checkpoints\", exist_ok=True)\n",
    "            with open(f\"pca_checkpoints/checkpoint_batch_{self.batch_count_pca}.pkl\", 'wb') as f:\n",
    "                pickle.dump({'pca': self.pca, 'scaler': self.scaler}, f)\n",
    "            print(f\"Saved checkpoint at batch {self.batch_count_pca}\")\n",
    "\n",
    "        print(f\"Processed batch {self.batch_count_pca}, shape: {vectors.shape} -> {reduced_vectors.shape}\")\n",
    "        return reduced_vectors\n",
    "\n",
    "    def save_final(self, model_path):\n",
    "        \"\"\"\n",
    "        Save the final PCA model to the specified path.\n",
    "\n",
    "        Args:\n",
    "            model_path: The file path to save the PCA model.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        Note: Change the model path as needed in the data_config.yml file.\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump({'pca': self.pca, 'scaler': self.scaler}, f)\n",
    "        print(f\"Final model saved to {model_path}. Total variance explained: {np.sum(self.pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "## For Single Input\n",
    "def load_pca_model(vectors, model_path=\"models/fusion/pca.pkl\"):\n",
    "    \"\"\"\n",
    "    Load a pre-trained PCA model and transform the input vectors.\n",
    "\n",
    "    Args:\n",
    "        vectors: The input data to transform.\n",
    "        model_path: The file path of the pre-trained PCA model.\n",
    "\n",
    "    Returns:\n",
    "        output: The PCA-transformed data.\n",
    "\n",
    "    Note: Change the model path as needed in the data_config.yml file (or set the path file as shown above). Can be used for the main program.\n",
    "    \"\"\"\n",
    "    model_path = Path(model_path)\n",
    "    pca = joblib.load(model_path)\n",
    "    return pca.transform(vectors)\n",
    "\n",
    "def l2vec_single_train(l2v, lyrics):\n",
    "    \"\"\"\n",
    "    Encode a single lyric string using the provided LLM2Vec model.\n",
    "\n",
    "    Args:\n",
    "        l2v: The LLM2Vec model for encoding lyrics.\n",
    "        lyrics: A single lyric string to encode.\n",
    "\n",
    "    Returns:\n",
    "        vectors: The vector representation of the lyrics.\n",
    "\n",
    "    \"\"\"\n",
    "    vectors = l2v.encode([lyrics]).detach().cpu().numpy()\n",
    "    return vectors\n",
    "\n",
    "# For Batch Processing\n",
    "def l2vec_train(l2v, lyrics_list):\n",
    "    \"\"\"\n",
    "    Encode a list of lyric strings using the provided LLM2Vec model.\n",
    "\n",
    "    Args:\n",
    "        l2v: The LLM2Vec model for encoding lyrics.\n",
    "        lyrics_list: A list of lyric strings to encode.\n",
    "    Returns:\n",
    "        vectors: The encoded vector representations of the lyrics.\n",
    "\n",
    "    Note: This function only encodes the lyrics and does not apply PCA reduction. The PCA reduction can be applied separately in the train.py module.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        vectors = l2v.encode(lyrics_list)  # lyrics_list: list of strings\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQZtRumO1SZI",
    "outputId": "5e0a6006-b09e-4358-dff7-7beeaa7dd29e"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/krislette/bach-or-bot.git\n",
    "%cd bach-or-bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBWkoCTu1bX5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the system path\n",
    "sys.path.append('/content/bach-or-bot/src')\n",
    "\n",
    "# Change the current directory to the project root\n",
    "os.chdir('/content/bach-or-bot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qam6ucE81QHn"
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import torch\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from src.spectttra.feature import FeatureExtractor\n",
    "from src.spectttra.spectttra import SpecTTTra, build_spectttra_from_cfg, load_frozen_spectttra\n",
    "\n",
    "# Shared variables for the model and setup, loaded only once and reused (cache)\n",
    "_PREDICTOR_LOCK = threading.Lock()\n",
    "_FEAT_EXT = None\n",
    "_MODEL = None\n",
    "_CFG = None\n",
    "_DEVICE = None\n",
    "\n",
    "\n",
    "def build_spectttra(cfg, device):\n",
    "    \"\"\"\n",
    "    Wrapper that builds SpecTTTra + FeatureExtractor and loads frozen checkpoint.\n",
    "    \"\"\"\n",
    "    feat_ext, model = build_spectttra_from_cfg(cfg, device)\n",
    "    model = load_frozen_spectttra(model, \"/content/bach-or-bot/models/spectttra/spectttra_frozen.pth\", device)\n",
    "    return feat_ext, model\n",
    "\n",
    "\n",
    "def _init_predictor_once():\n",
    "    \"\"\"\n",
    "    Initialize and cache FeatureExtractor and SpecTTTra once per process.\n",
    "\n",
    "    Ensures thread-safe, one-time initialization of the feature extractor and\n",
    "    transformer model, including moving them to the appropriate device.\n",
    "\n",
    "    This function also sets default configurations for audio,\n",
    "    mel-spectrogram extraction, and model architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    global _FEAT_EXT, _MODEL, _CFG, _DEVICE\n",
    "\n",
    "    if _MODEL is not None and _FEAT_EXT is not None:\n",
    "        return\n",
    "\n",
    "    with _PREDICTOR_LOCK:\n",
    "        if _MODEL is not None and _FEAT_EXT is not None:\n",
    "            return\n",
    "\n",
    "        # Configurations of best performing variant for 120s\n",
    "        cfg = SimpleNamespace(\n",
    "            audio=SimpleNamespace(sample_rate=16000, max_time=120, max_len=16000 * 120),\n",
    "            melspec=SimpleNamespace(\n",
    "                n_fft=2048,\n",
    "                hop_length=512,\n",
    "                win_length=2048,\n",
    "                n_mels=128,\n",
    "                f_min=20,\n",
    "                f_max=8000,\n",
    "                power=2,\n",
    "                top_db=80,\n",
    "                norm=\"mean_std\",\n",
    "            ),\n",
    "            model=SimpleNamespace(\n",
    "                embed_dim=384,\n",
    "                num_heads=6,\n",
    "                num_layers=12,\n",
    "                t_clip=3,\n",
    "                f_clip=1,\n",
    "                pre_norm=True,\n",
    "                pe_learnable=True,\n",
    "                pos_drop_rate=0.1,\n",
    "                attn_drop_rate=0.1,\n",
    "                proj_drop_rate=0.0,\n",
    "                mlp_ratio=2.67,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        feat_ext, model = build_spectttra(cfg, device)\n",
    "        feat_ext.to(device)\n",
    "\n",
    "        # Move model to device (GPU if available) and allow faster inference with mixed precision\n",
    "        model.to(device).eval()\n",
    "\n",
    "        # Cache\n",
    "        _FEAT_EXT, _MODEL, _CFG, _DEVICE = feat_ext, model, cfg, device\n",
    "\n",
    "\n",
    "def spectttra_predict(audio_tensor):\n",
    "    \"\"\"\n",
    "    Run single-input inference with SpecTTTra.\n",
    "\n",
    "    Args:\n",
    "        audio_tensor (torch.Tensor): Input waveform of shape (1, num_samples).\n",
    "            Must already be preprocessed including resampled to the target sampling rate (16 kHz).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray:\n",
    "            1D embedding vector of shape (embed_dim,). The embedding is obtained\n",
    "            by mean-pooling the transformer token outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    global _FEAT_EXT, _MODEL, _CFG, _DEVICE\n",
    "\n",
    "    _init_predictor_once()\n",
    "\n",
    "    device = _DEVICE\n",
    "    feat_ext = _FEAT_EXT\n",
    "    model = _MODEL\n",
    "    cfg = _CFG\n",
    "\n",
    "    # Move waveform to device but keep float for mel extraction\n",
    "    waveform = audio_tensor.to(device).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Extract mel-spectrogram\n",
    "        melspec = feat_ext(waveform)\n",
    "\n",
    "        # Ensure melspec shape matches model's expectation ---\n",
    "        expected_frames = model.input_temp_dim  # expected_frames is 3744\n",
    "        if melspec.shape[2] > expected_frames:\n",
    "            melspec = melspec[:, :, :expected_frames]\n",
    "        elif melspec.shape[2] < expected_frames:\n",
    "            padding = expected_frames - melspec.shape[2]\n",
    "            melspec = torch.nn.functional.pad(melspec, (0, padding))\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            with torch.cuda.amp.autocast(enabled=True):\n",
    "                tokens = model(melspec)\n",
    "                pooled = tokens.mean(dim=1)\n",
    "        else:\n",
    "            tokens = model(melspec)\n",
    "            pooled = tokens.mean(dim=1)\n",
    "\n",
    "    out = pooled.squeeze(0).cpu().numpy()\n",
    "    return out\n",
    "\n",
    "\n",
    "def spectttra_train(audio_tensors):\n",
    "    \"\"\"\n",
    "    Efficient batch inference with SpecTTTra (GPU-optimized and pad-safe).\n",
    "    Args:\n",
    "        audio_tensors (list[torch.Tensor]): List of tensors (1, num_samples) or (num_samples,)\n",
    "    Returns:\n",
    "        np.ndarray of shape (batch_size, embed_dim)\n",
    "    \"\"\"\n",
    "    global _FEAT_EXT, _MODEL, _CFG, _DEVICE\n",
    "    _init_predictor_once()\n",
    "\n",
    "    if not audio_tensors:\n",
    "        return np.empty((0, _CFG.model.embed_dim))\n",
    "\n",
    "    # Normalize shape and get max length\n",
    "    normalized = []\n",
    "    max_len = max(t.numel() for t in audio_tensors)\n",
    "\n",
    "    for t in audio_tensors:\n",
    "        # Ensure each tensor is shape (1, num_samples)\n",
    "        if t.ndim == 1:\n",
    "            t = t.unsqueeze(0)\n",
    "        elif t.ndim > 2:\n",
    "            raise ValueError(f\"Unexpected tensor shape: {t.shape}\")\n",
    "\n",
    "        # Pad shorter tensors to max length\n",
    "        pad_len = max_len - t.shape[-1]\n",
    "        if pad_len > 0:\n",
    "            t = torch.nn.functional.pad(t, (0, pad_len))\n",
    "        normalized.append(t)\n",
    "\n",
    "    # Stack into batch (B, 1, num_samples)\n",
    "    batch_waveforms = torch.cat(normalized, dim=0).to(_DEVICE).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        melspec = _FEAT_EXT(batch_waveforms)  # (B, n_mels, n_frames)\n",
    "\n",
    "        if _DEVICE.type == \"cuda\":\n",
    "            with torch.cuda.amp.autocast(enabled=True):\n",
    "                tokens = _MODEL(melspec)      # (B, num_tokens, embed_dim)\n",
    "                pooled = tokens.mean(dim=1)   # (B, embed_dim)\n",
    "        else:\n",
    "            tokens = _MODEL(melspec)\n",
    "            pooled = tokens.mean(dim=1)\n",
    "\n",
    "    return pooled.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SP_J5mr_Ef5w",
    "outputId": "4707d36a-3215-453b-bcf2-4bb8e8e6b162"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import threading\n",
    "import numpy as np\n",
    "import torch\n",
    "from flask import Flask, request, jsonify\n",
    "from pyngrok import ngrok, conf\n",
    "\n",
    "# Import your SpecTTTra functions\n",
    "from __main__ import spectttra_predict, spectttra_train\n",
    "from __main__ import l2vec_single_train, l2vec_train, l2v\n",
    "\n",
    "# Setup ngrok Authentication\n",
    "\n",
    "print(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
    "conf.get_default().auth_token = getpass.getpass(\"Ngrok Authtoken: \")\n",
    "\n",
    "\n",
    "# Initialize Flask + ngrok\n",
    "\n",
    "app = Flask(__name__)\n",
    "port = 5000\n",
    "\n",
    "public_url = ngrok.connect(port).public_url\n",
    "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")\n",
    "app.config[\"BASE_URL\"] = public_url\n",
    "\n",
    "# Flask Routes\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return \"\"\"\n",
    "    <h1>L2Vec + SpecTTTra Flask API is running</h1>\n",
    "    <p>Available endpoints:</p>\n",
    "    <ul>\n",
    "      <li>POST /single → {'lyrics': 'string'}</li>\n",
    "      <li>POST /batch → {'lyrics_list': ['song1', 'song2', ...]}</li>\n",
    "      <li>POST /spectttra/predict → {'audio': [[...]]}</li>\n",
    "      <li>POST /spectttra/train → {'audios': [[[...]], [[...]], ...]}</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "\n",
    "# --- L2V single lyric ---\n",
    "@app.route(\"/single\", methods=[\"POST\"])\n",
    "def single():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        if \"lyrics\" not in data:\n",
    "            return jsonify({\"error\": \"Missing 'lyrics' field\"}), 400\n",
    "\n",
    "        lyrics = data[\"lyrics\"]\n",
    "        vectors = l2vec_single_train(l2v, lyrics)\n",
    "        return jsonify({\"vectors\": vectors.tolist()})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "# --- L2V batch lyrics ---\n",
    "@app.route(\"/batch\", methods=[\"POST\"])\n",
    "def batch():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        if \"lyrics_list\" not in data:\n",
    "            return jsonify({\"error\": \"Missing 'lyrics_list' field\"}), 400\n",
    "\n",
    "        lyrics_list = data[\"lyrics_list\"]\n",
    "        vectors = l2vec_train(l2v, lyrics_list)\n",
    "        return jsonify({\"vectors\": vectors.tolist()})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "# --- SpecTTTra: single audio ---\n",
    "@app.route(\"/spectttra/predict\", methods=[\"POST\"])\n",
    "def predict_audio():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        if \"audio\" not in data:\n",
    "            return jsonify({\"error\": \"Missing 'audio' field\"}), 400\n",
    "\n",
    "        # Convert to tensor with shape (1, num_samples)\n",
    "        audio_array = np.array(data[\"audio\"], dtype=np.float32)\n",
    "        if audio_array.ndim != 1:\n",
    "            return jsonify({\"error\": \"Audio must be 1D list of floats\"}), 400\n",
    "\n",
    "        audio_tensor = torch.tensor(audio_array).unsqueeze(0)  # (1, num_samples)\n",
    "        embedding = spectttra_predict(audio_tensor)\n",
    "        return jsonify({\"embedding\": embedding.tolist()})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "# --- SpecTTTra: batch audio ---\n",
    "@app.route(\"/spectttra/train\", methods=[\"POST\"])\n",
    "def train_audio():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        if \"audios\" not in data:\n",
    "            return jsonify({\"error\": \"Missing 'audios' field\"}), 400\n",
    "\n",
    "        audio_tensors = []\n",
    "        for a in data[\"audios\"]:\n",
    "            arr = np.array(a, dtype=np.float32)\n",
    "            if arr.ndim == 1:\n",
    "                arr = np.expand_dims(arr, axis=0)  # (1, num_samples)\n",
    "            tensor = torch.tensor(arr)\n",
    "            audio_tensors.append(tensor)\n",
    "\n",
    "        embeddings = spectttra_train(audio_tensors)\n",
    "        return jsonify({\"embeddings\": embeddings.tolist()})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "\n",
    "# Start Flask in Background\n",
    "\n",
    "threading.Thread(target=app.run, kwargs={\"use_reloader\": False}).start()\n",
    "\n",
    "print(\"\\nYour Colab backend is live!\")\n",
    "print(f\"L2V endpoints:\\n  {public_url}/single\\n  {public_url}/batch\")\n",
    "print(f\"SpecTTTra endpoints:\\n  {public_url}/spectttra/predict\\n  {public_url}/spectttra/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tL65x7F8HGU1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def flush_gpu_cache():\n",
    "    \"\"\"Flushes the GPU cache.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"GPU cache flushed.\")\n",
    "    else:\n",
    "        print(\"No GPU available.\")\n",
    "\n",
    "# Example usage: Call this function whenever you want to flush the cache\n",
    "# flush_gpu_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
