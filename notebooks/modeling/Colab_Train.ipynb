{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SGhfPAi9mSxt",
    "outputId": "e7e4dc4f-02a3-4102-f44f-9de1d69f811f"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/krislette/bach-or-bot.git\n",
    "%cd bach-or-bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w0Ih3ejP6-yg",
    "outputId": "c356e19b-8196-4c18-93b6-8108a8104edd"
   },
   "outputs": [],
   "source": [
    " pip install  torch llm2vec librosa pandas soundfile torchaudio peft timm pyyaml torchao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YRQPJUjQGphW",
    "outputId": "19274312-2d3d-4535-89c8-e55be4639aae"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "b459b873",
    "outputId": "a4011323-a210-4bcc-f1b8-de8520dc713d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = \"/content/drive/MyDrive/data/external/1k_dataset.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV file loaded successfully.\")\n",
    "    display(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the CSV file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d33d2a88",
    "outputId": "6600f6c5-b3b6-4f01-bf0d-41f25c4935fc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the base directory where the folders '01' to '05' are located\n",
    "# IMPORTANT: Replace '/path/to/your/base/directory' with the actual path on your system\n",
    "base_directory = '/content/drive/MyDrive/data/raw' # Assuming the folders are here\n",
    "\n",
    "def check_file_existence(directory):\n",
    "    \"\"\"Checks if a file exists at the given path including the base directory.\"\"\"\n",
    "    full_path = os.path.join(base_directory, directory)\n",
    "    return os.path.exists(full_path)\n",
    "\n",
    "# Apply the function to the 'directory' column and create a new column 'file_exists'\n",
    "df['file_exists'] = df['directory'].apply(check_file_existence)\n",
    "\n",
    "# Report the results\n",
    "files_found = df['file_exists'].sum()\n",
    "files_missing = len(df) - files_found\n",
    "\n",
    "print(f\"Total files listed: {len(df)}\")\n",
    "print(f\"Files found: {files_found}\")\n",
    "print(f\"Files missing: {files_missing}\")\n",
    "\n",
    "if files_missing > 0:\n",
    "    print(\"\\nMissing files:\")\n",
    "    display(df[~df['file_exists']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669,
     "referenced_widgets": [
      "fd8cd942fdb14a05844aab936c51d636",
      "20e46dd138504ace944379d0c7369487",
      "1c09726a6ffe4a4cb1c52121e88d2dff",
      "3069f536b1054dce8d9029f702a07f65",
      "8a07ca88a46948dcb655b12bbe45a448",
      "0dbefd69f3f84c838856baf75d340a16",
      "3db6cbc5c8d2432c9a2a0bb5a2287eb6",
      "2798deb8dd514cc99a25717ae01b092a",
      "d8876deb3f30473f9023904ff3491796",
      "70c0240c2eaa4558903dae201712bb84",
      "5a7288f827164821970d51564f13160e",
      "ac0fc155a5444451b0476f297c61850d",
      "902f070efd8a42c3adf16bddf0e40196",
      "6a169a4bc11f4f7c8442619be1e8425a",
      "67c9c9637901463cb005c24d221ec6aa",
      "f5d281fe66064b11a9ce142e96d286e6",
      "98c13863028d4fc4a98589d4f52304ac",
      "b7cd39416a7347e1bd5559ab7dabbf33",
      "9540bedf3f43405da418c9106c067eae",
      "7e57875a5ea4411c95fed796f7cdb6e7",
      "4b749df2a89a4714a6a3d0bf5e1daee6",
      "e279de545a7b4cd5affec57b6ede8769",
      "e47d4d32e7244a8a9279a98e9ad1b54c",
      "8587191ff22142a5a72e1c2f8188fa1f",
      "ceb668869c4f4fa7a624a28eb83f395b",
      "4374de41beec463e880c7f784b86c47a",
      "efdf7da457b1458e9a0a06a6fde3a636",
      "7cd07f5afc334f849c0843098e821d83",
      "924eb390fbe049ff8e226098756bddfc",
      "bd4e111f68274d8ab9ca31be3ce257f7",
      "2e559bbb2f9941f8b76cedf76f7a0b6f",
      "d4c2a4cb66584958ba9a3980611d2590",
      "6abbeb1ecfd24841a3bc1a618eb06ace",
      "4fe24a4a3a2c4f8a98c965aa518b80f7",
      "f9adfb74a8324e6d99cc1b34b4eac6bc",
      "82767557a77a4087bc23546bf87a721b",
      "93aab67e387d40b3a12a5200c5c11b02",
      "10fde5502e2b4d799b68930551711f63",
      "1d0fdc924c5e4f84b82d38214f121d1a",
      "add65aaa6269480aab4b0ef73a5a579c",
      "e5f9315c7e894ede9b0eb5530517a257",
      "fc2c5f3746d246a9b0a5cabb16f204f0",
      "63e94ae5341b4409a5c2016f28939d98",
      "4f1466dd409d4401a7fe2444fdcbdd74",
      "e68f8fa07caf4fe08a2627e483ae524f",
      "248abf222da842568de9cb261ca30cc2",
      "e7f76bcc8bc242808477822ee9f8cbde",
      "46af34c8dc3546ef96195bc24dbc44d9",
      "7738a205064d4803a77a1cd929be4561",
      "008ea570f47d41e09b15d98c4336414c",
      "e2032889de104884a971803a878d42f3",
      "2bc3d3809dd744ea8b3de5ac79bb0e8d",
      "88dd292fa69843de869cd1e0a5a5ab35",
      "0ddcbec0748a45bbaab4524c6fac8eef",
      "68009e1fd68744749f11e314c93b6271",
      "5a88916953ec443bafde1291950b9b98",
      "2b675d0c8d964428a2ca0434eba516ad",
      "efb923e6645b45f1bf8a3bc328447251",
      "eb2e35bf61df4912b5ff968dd7f29ac7",
      "1f22c996295c4920a63850dbe491134d",
      "fc487599f4264ec7836ccc667214baf5",
      "4e8b53df127341b9b750e6a877a6019e",
      "418e84164aae41179a7ef78c9c35ef46",
      "29ff7f0ea11e4f08899aad41044d3d73",
      "01ffc0eaa1ed4c4d80f4a1f39405d0d2",
      "ea3be9694ea042969662e76db00683ed",
      "ea983611ac0349ce87ead28a6de64cfb",
      "f7b9568cb11a4e5b82d9ea5bb4601497",
      "b08147d17c5e4ecba14ceb3a4c96be63",
      "22c9d1db056b41c8bb780693bca4afa4",
      "b805de05aea84e4994899fe0cdb1b531",
      "fe6209f2747240d78182990aa9bc8072",
      "36115a7976624fca90b675dc47024bbc",
      "77a664309a8e45d5b301a3962ad0ed10",
      "c95e0462f0964c489a564c54fa576405",
      "ed5dcf7ce8104560ac3f3006bb9b6733",
      "423be55402234522a4945b67afd06de2",
      "6d5934db335a4a74a7946313561229b8",
      "bdc63288cb91480fa1a5dc71263e9e6c",
      "649a3d2b880b40348cfb964f2726aefd",
      "af4833e70d7b454990dfde55eb86d037",
      "f15660ba99694df7a464cf7149ecff57",
      "db67fcec062c4ed1a32b01f0fc1d74af",
      "e550576d93534d0aaeb36d92b44ecb82",
      "724c2c79263c480aa2c724e509f6f2c1",
      "74406883cabd40898063bbbbd8faacd5",
      "9874c466d9e747fc8d846cf9e9b3b68b",
      "69cbde8cd45345288364b1044819a208",
      "2384d010ae3b4413b0f0e1f5df46ba70",
      "204becc0c8d74733b410fc835fa157c5",
      "d57cf224149247a997dde078ff1193f0",
      "cd7ea25232234a4292c048f0427af8f7",
      "98ce706ad778431e970d9715770ad19f",
      "baba0e6716b8409496204312bbf939e5",
      "dab2dfb0334849a88fd550b7b6c914e1",
      "e98cd07da64f4ddb93f5f04224819709",
      "a3c1f825f820435288312e2b2da7a55c",
      "fe8823a3aefb43d4b27e2862ec71ee5b",
      "415994283b3b42ea8bba5568a4f7f5ee",
      "2c882e3b4329428c82aee0ef611e6540",
      "e060f55102974beca591872437b2b278",
      "6d3302f4b925448a8fafe3d4a6c508d8",
      "809829e98608414483a9d7967af8e004",
      "47682d9c75eb4bbfb9fb4b048aa4e7c0",
      "ed8fe03415334afa9b3e328ca83d8a0e",
      "d83c69b1bb804a4888e2491b1e65ff5a",
      "8b451131cd2849ccb81a809af7daa86e",
      "f58ffca784584375b297c982195f04ce",
      "dc90b753ec72494d998150170eece5c1",
      "917d88536a944188814b8e3ffdd42f9b"
     ]
    },
    "id": "ADgns8kbnpPO",
    "outputId": "d1363bf5-8663-4984-8a73-2c2a22df5b5e"
   },
   "outputs": [],
   "source": [
    "from llm2vec import LLM2Vec\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from peft import PeftModel\n",
    "from torchao.quantization import quantize_, Int8WeightOnlyConfig\n",
    "import torch\n",
    "\n",
    "access_token = REDACTED_TOKEN\n",
    "\n",
    "model_id = \"McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding = True, truncation = True, max_length = 512)\n",
    "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU\")\n",
    "# GPU path: use bf16 for speed\n",
    "    model = AutoModel.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\",\n",
    "    token=access_token,\n",
    ")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "# CPU path: use float32 first, then quantize\n",
    "    model = AutoModel.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=config,\n",
    "    torch_dtype=torch.float32,   # quantization requires fp32\n",
    "    device_map=\"cpu\",\n",
    "    token=access_token,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        from torchao.quantization import quantize_\n",
    "        print(\"[INFO] Applying torchao quantization for CPU...\")\n",
    "        quant_config = Int8WeightOnlyConfig(group_size=None)\n",
    "        print(\"[INFO] Applying torchao quantization with Int8WeightOnlyConfig...\")\n",
    "        quantize_(model, quant_config)\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"[WARNING] torchao not installed. Run: pip install torchao\")\n",
    "        print(\"[WARNING] Falling back to non-quantized CPU model.\")\n",
    "\n",
    "l2v = LLM2Vec(model, tokenizer, pooling_mode=\"mean\", max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "45d685e4e1d5449498b48f5b0f8b2fad",
      "bb12d28ccd7044ab8b7d8bfc075b67df",
      "9c552559b29e48188e2d1fe1d0f20f47",
      "02cb0e410f204a1f99f1635194879745",
      "dfe1562b25d14487b52379938179afea",
      "7d87d7e332864835a61ae2e33b080274",
      "d5f651f9bb664fb59c41344263f61f7a",
      "13987853c11c4ab885f4228c32863dde",
      "fcb9d0c928354ffaaffef16ec28da770",
      "be9ae926948842fcbb56cb73eafd1257",
      "55e37d607e744be3ab7db240115fc92d",
      "e9ae7a57d84749d1a2c2e5129571f5a8",
      "583f5167188247f79b20fcaa6cc209be",
      "bf928c56a6ab450da7357ec1f5ea831e",
      "cc5360cb44e946f08e0ad826b4182c35",
      "73a9bd0678464b93bfe0e3d7e2107e9f",
      "5a04b6c01be44ae89973cf30b1e6e628",
      "ecff66b30eb94951843d34925ec1c36b",
      "aaf1d17b96124b7f8365cdb881d1f596",
      "8e7e84b0e5274b0a9254fff4643f0690",
      "badb0b8c41644da5ae905ba4e8c47706",
      "b112dcf304b7489bbde5669729b0a02c"
     ]
    },
    "id": "lCR8_IGePX20",
    "outputId": "1b111a92-03cd-447a-dc34-dff5f7364a54"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the system path\n",
    "sys.path.append('/content/bach-or-bot/src')\n",
    "\n",
    "# Change the current directory to the project root\n",
    "os.chdir('/content/bach-or-bot')\n",
    "\n",
    "# Import the necessary functions from the script\n",
    "from src.preprocessing.preprocessor import dataset_read, bulk_preprocessing\n",
    "from src.spectttra.spectttra_trainer import spectttra_train\n",
    "from pathlib import Path\n",
    "from src.utils.config_loader import DATASET_NPZ, PCA_MODEL, RAW_DATASET_NPZ\n",
    "from src.utils.dataset import dataset_scaler\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import gc\n",
    "import joblib\n",
    "\n",
    "# Initialize PCA and StandardScaler globally for training\n",
    "_pca_trainer = None\n",
    "\n",
    "# Initialize PCA and StandardScaler globally for training\n",
    "_pca_trainer = None\n",
    "\n",
    "class SimplePCATrainer:\n",
    "    \"\"\"\n",
    "    A simple PCA trainer that uses IncrementalPCA to fit data in batches.\n",
    "    It saves checkpoints every 5 batches and can save the final model.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Attributes:\n",
    "        pca: The IncrementalPCA model.\n",
    "        scaler: StandardScaler for normalizing data.\n",
    "        fitted: Boolean indicating if the model has been initialized.\n",
    "        batch_count_pca: Counter for the number of batches processed.\n",
    "\n",
    "    Methods:\n",
    "        process_batch(vectors): Processes a batch of vectors, fits the PCA model incrementally.\n",
    "        save_final(model_path): Saves the final PCA model to the specified path.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the trainer\n",
    "    def __init__(self):\n",
    "        self.pca = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.fitted = False\n",
    "        self.batch_count_pca = 0\n",
    "\n",
    "    def _determine_optimal_components(self, vectors):\n",
    "        \"\"\"\n",
    "        Determine the optimal number of PCA components to retain 95% variance.\n",
    "\n",
    "        Args:\n",
    "            vectors: The input data to analyze.\n",
    "        Returns:\n",
    "            n_components: The optimal number of components.\n",
    "        \"\"\"\n",
    "        temp_pca = IncrementalPCA()\n",
    "        temp_pca.fit(vectors)\n",
    "        cumsum_var = np.cumsum(temp_pca.explained_variance_ratio_)\n",
    "        n_comp_95 = np.argmax(cumsum_var >= 0.95) + 1\n",
    "        return min(n_comp_95, vectors.shape[1] // 2)\n",
    "\n",
    "    def process_batch(self, vectors):\n",
    "        \"\"\"\n",
    "        Process a batch of vectors, fitting the PCA model incrementally.\n",
    "\n",
    "        Args:\n",
    "            vectors: The input data batch to process.\n",
    "        Returns:\n",
    "            reduced_vectors: The PCA-transformed data.\n",
    "\n",
    "        Note: This method saves a checkpoint every 5 batches.\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            # First batch - initialize everything\n",
    "            n_components = self._determine_optimal_components(vectors)\n",
    "            self.pca = IncrementalPCA(n_components=n_components, batch_size=500)\n",
    "            self.scaler.fit(vectors)\n",
    "            self.fitted = True\n",
    "            print(f\"Initialized PCA with {n_components} components\")\n",
    "\n",
    "        # Process batch\n",
    "        vectors_scaled = self.scaler.transform(vectors)\n",
    "        self.pca.partial_fit(vectors_scaled)\n",
    "        reduced_vectors = self.pca.transform(vectors_scaled)\n",
    "\n",
    "        self.batch_count_pca += 1\n",
    "\n",
    "        # Save checkpoint every 5 batches\n",
    "        if self.batch_count_pca % 5 == 0:\n",
    "            os.makedirs(\"pca_checkpoints\", exist_ok=True)\n",
    "            with open(f\"pca_checkpoints/checkpoint_batch_{self.batch_count_pca}.pkl\", 'wb') as f:\n",
    "                pickle.dump({'pca': self.pca, 'scaler': self.scaler}, f)\n",
    "            print(f\"Saved checkpoint at batch {self.batch_count_pca}\")\n",
    "\n",
    "        print(f\"Processed batch {self.batch_count_pca}, shape: {vectors.shape} -> {reduced_vectors.shape}\")\n",
    "        return reduced_vectors\n",
    "\n",
    "    def save_final(self, model_path):\n",
    "        \"\"\"\n",
    "        Save the final PCA model to the specified path.\n",
    "\n",
    "        Args:\n",
    "            model_path: The file path to save the PCA model.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        Note: Change the model path as needed in the data_config.yml file.\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump({'pca': self.pca, 'scaler': self.scaler}, f)\n",
    "        print(f\"Final model saved to {model_path}. Total variance explained: {np.sum(self.pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "## For Single Input\n",
    "def load_pca_model(vectors, model_path=\"models/fusion/pca.pkl\"):\n",
    "    \"\"\"\n",
    "    Load a pre-trained PCA model and transform the input vectors.\n",
    "\n",
    "    Args:\n",
    "        vectors: The input data to transform.\n",
    "        model_path: The file path of the pre-trained PCA model.\n",
    "\n",
    "    Returns:\n",
    "        output: The PCA-transformed data.\n",
    "\n",
    "    Note: Change the model path as needed in the data_config.yml file (or set the path file as shown above). Can be used for the main program.\n",
    "    \"\"\"\n",
    "    model_path = Path(model_path)\n",
    "    pca = joblib.load(model_path)\n",
    "    return pca.transform(vectors)\n",
    "\n",
    "def l2vec_single_train(l2v, lyrics):\n",
    "    \"\"\"\n",
    "    Encode a single lyric string using the provided LLM2Vec model.\n",
    "\n",
    "    Args:\n",
    "        l2v: The LLM2Vec model for encoding lyrics.\n",
    "        lyrics: A single lyric string to encode.\n",
    "\n",
    "    Returns:\n",
    "        vectors: The vector representation of the lyrics.\n",
    "\n",
    "    \"\"\"\n",
    "    vectors = l2v.encode([lyrics]).detach().cpu().numpy()\n",
    "    return vectors\n",
    "\n",
    "# For Batch Processing\n",
    "def l2vec_train(l2v, lyrics_list):\n",
    "    \"\"\"\n",
    "    Encode a list of lyric strings using the provided LLM2Vec model.\n",
    "\n",
    "    Args:\n",
    "        l2v: The LLM2Vec model for encoding lyrics.\n",
    "        lyrics_list: A list of lyric strings to encode.\n",
    "    Returns:\n",
    "        vectors: The encoded vector representations of the lyrics.\n",
    "\n",
    "    Note: This function only encodes the lyrics and does not apply PCA reduction. The PCA reduction can be applied separately in the train.py module.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        vectors = l2v.encode(lyrics_list)  # lyrics_list: list of strings\n",
    "    return vectors\n",
    "\n",
    "def train_pipeline():\n",
    "    \"\"\"\n",
    "    Training script which includes preprocessing, feature extraction, and training the MLP model.\n",
    "\n",
    "    The train pipeline saves the train dataset in an .npz format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Instantiate X and Y vectors\n",
    "    X, Y = None, None\n",
    "\n",
    "    dataset_path = Path(DATASET_NPZ)\n",
    "\n",
    "    if dataset_path.exists():\n",
    "        print(\"Training dataset already exists. Loading file...\")\n",
    "\n",
    "        loaded_data = np.load(DATASET_NPZ)\n",
    "        X = loaded_data[\"X\"]\n",
    "        Y = loaded_data[\"Y\"]\n",
    "    else:\n",
    "        print(\"Training dataset does not exist. Processing data...\")\n",
    "        # Get batches from dataset and return full Y labels\n",
    "        batches, Y = dataset_read(batch_size=2)\n",
    "        batch_count = 1\n",
    "\n",
    "        # Instantiate LLM2Vec and PCA model\n",
    "        llm2vec_model = l2v\n",
    "\n",
    "        # Preallocate spaces for both audio and lyric vectors to reduce memory overhead\n",
    "        audio_vectors = np.zeros((len(Y), 384), dtype=np.float32)\n",
    "        lyric_vectors = np.zeros((len(Y), 2048), dtype=np.float32)\n",
    "\n",
    "        start_idx = 0\n",
    "        for batch in batches:\n",
    "            print(f\"Bulk Preprocessing - Batch {batch_count}.\")\n",
    "            audio, lyrics = bulk_preprocessing(batch, batch_count)\n",
    "            batch_count += 1\n",
    "\n",
    "            # Call the train methods for both SpecTTTra and LLM2Vec\n",
    "            print(\"Starting SpecTTTra feature extraction...\")\n",
    "            audio_features = spectttra_train(audio)\n",
    "\n",
    "            print(\"Starting LLM2Vec feature extraction...\")\n",
    "            lyrics_features = l2vec_train(llm2vec_model, lyrics)\n",
    "\n",
    "            batch_size = audio_features.shape[0]\n",
    "\n",
    "            # Store the results on preallocated spaces\n",
    "            audio_vectors[start_idx:start_idx + batch_size, :] = audio_features\n",
    "            lyric_vectors[start_idx:start_idx + batch_size, :] = lyrics_features\n",
    "\n",
    "            # Delete stored instance for next batch to remove overhead\n",
    "            del audio, lyrics, audio_features, lyrics_features\n",
    "\n",
    "        # Save both X and Y to an .npz file for easier loading\n",
    "        print(\"Saving dataset for future testing...\")\n",
    "        np.savez(RAW_DATASET_NPZ, audio=audio_vectors, lyrics=lyric_vectors, labels=Y)\n",
    "\n",
    "        # Run standard scaling on audio and lyrics separately\n",
    "        print(\"Running standard scaling for audio and lyrics...\")\n",
    "        audio_vectors, lyric_vectors = dataset_scaler(audio_vectors, lyric_vectors)\n",
    "\n",
    "        # Run PCA per batch to reduce GPU overhead\n",
    "        ipca = IncrementalPCA(n_components=256)\n",
    "        batch_size = 1000  # Adjust depending on memory\n",
    "\n",
    "        # Fit IPCA in batches\n",
    "        for i in range(0, lyric_vectors.shape[0], batch_size):\n",
    "            ipca.partial_fit(lyric_vectors[i:i + batch_size])\n",
    "\n",
    "        # Transform in batches\n",
    "        lyric_vectors_reduced = np.zeros((lyric_vectors.shape[0], 256), dtype=np.float32)\n",
    "        for i in range(0, lyric_vectors.shape[0], batch_size):\n",
    "            lyric_vectors_reduced[i:i + batch_size, :] = ipca.transform(lyric_vectors[i:i + batch_size])\n",
    "\n",
    "        # Save IncrementalPCA model\n",
    "        joblib.dump(ipca, \"models/fusion/incremental_pca.pkl\")\n",
    "        lyric_vectors = lyric_vectors_reduced\n",
    "\n",
    "        # Run standard scaling on audio and lyrics separately\n",
    "        print(\"Running standard scaling for audio and lyrics...\")\n",
    "        _, lyric_vectors = dataset_scaler(audio_vectors, lyric_vectors)\n",
    "\n",
    "        # Concatenate audio features and reduced lyrics features\n",
    "        X = np.concatenate([audio_vectors, lyric_vectors], axis=1)\n",
    "        print(f\"Audio and Lyrics Concatenated. Final features shape: {X.shape}\")\n",
    "\n",
    "        # Convert label list into np.array\n",
    "        Y = np.array(Y)\n",
    "\n",
    "        # Save both X and Y to an .npz file for easier loading\n",
    "        print(\"Saving dataset for future testing...\")\n",
    "        np.savez(DATASET_NPZ, X=X, Y=Y)\n",
    "\n",
    "print(\"Executing train_pipeline...\")\n",
    "train_pipeline()\n",
    "print(\"train_pipeline execution finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zT_N8ussqBEb",
    "outputId": "a937e904-3163-4585-fa4b-4ee414403765"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def flush_gpu_cache():\n",
    "    \"\"\"Flushes the GPU cache to free up memory.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"GPU cache flushed.\")\n",
    "    else:\n",
    "        print(\"No GPU available to flush cache.\")\n",
    "\n",
    "# Example usage:\n",
    "flush_gpu_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
