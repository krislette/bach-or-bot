{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# SpecTTTra: A Mock Illustration of Audio Input to Spectral & Temporal Clips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Load audio and extract Mel Spectrogram\n",
    "\n",
    "We use `librosa` to load an audio file and extract its mel spectrogram. The mel spectrogram is a time-frequency representation that captures both the spectral (frequency) and temporal (time) characteristics of the audio signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load an example audio file\n",
    "audio_path = 'audio/folded.mp3' \n",
    "sample_rate = 16000\n",
    "max_time = 5 \n",
    "max_len = sample_rate * max_time\n",
    "\n",
    "# Load audio and trim/pad to max_len\n",
    "audio, sr = librosa.load(audio_path, sr=sample_rate)\n",
    "if len(audio) < max_len:\n",
    "    audio = np.pad(audio, (0, max_len - len(audio)), mode='constant')\n",
    "else:\n",
    "    audio = audio[:max_len]\n",
    "\n",
    "# Extract mel spectrogram\n",
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "n_mels = 128\n",
    "melspec = librosa.feature.melspectrogram(\n",
    "    y=audio,\n",
    "    sr=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    hop_length=hop_length,\n",
    "    n_mels=n_mels,\n",
    "    fmin=20,\n",
    "    fmax=8000,\n",
    "    power=2\n",
    ")\n",
    "melspec_db = librosa.power_to_db(melspec, ref=np.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Divide the Spectrogram into Temporal and Spectral Clips\n",
    "\n",
    "SpecTTTra divides the spectrogram into two types of clips:\n",
    "- **Temporal Clips:** Slices along the time axis, capturing short time windows across all frequencies.\n",
    "- **Spectral Clips:** Slices along the frequency axis, capturing frequency bands across all time frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define clip sizes\n",
    "t_clip = 5  # temporal clip size (frames)\n",
    "f_clip = 3  # spectral clip size (mel bins)\n",
    "\n",
    "# Calculate number of clips\n",
    "num_temporal_clips = (melspec_db.shape[1] - t_clip) // t_clip + 1\n",
    "num_spectral_clips = (melspec_db.shape[0] - f_clip) // f_clip + 1\n",
    "\n",
    "# Extract temporal clips\n",
    "temporal_clips = []\n",
    "for i in range(num_temporal_clips):\n",
    "    start = i * t_clip\n",
    "    end = start + t_clip\n",
    "    clip = melspec_db[:, start:end]\n",
    "    temporal_clips.append(clip)\n",
    "\n",
    "# Extract spectral clips\n",
    "spectral_clips = []\n",
    "for i in range(num_spectral_clips):\n",
    "    start = i * f_clip\n",
    "    end = start + f_clip\n",
    "    clip = melspec_db[start:end, :]\n",
    "    spectral_clips.append(clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Visualization\n",
    "\n",
    "We visualize the mel spectrogram and overlay rectangles to show the first few temporal and spectral clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(melspec_db, aspect='auto', origin='lower', cmap='magma')\n",
    "plt.title('Mel Spectrogram with Temporal and Spectral Clips')\n",
    "plt.xlabel('Time Frames')\n",
    "plt.ylabel('Mel Frequency Bins')\n",
    "\n",
    "# Overlay temporal clips (vertical rectangles)\n",
    "for i in range(min(10, num_temporal_clips)):\n",
    "    start = i * t_clip\n",
    "    plt.gca().add_patch(plt.Rectangle((start, 0), t_clip, n_mels, edgecolor='cyan', facecolor='none', lw=2, label='Temporal Clip' if i==0 else None))\n",
    "\n",
    "# Overlay spectral clips (horizontal rectangles)\n",
    "for i in range(min(10, num_spectral_clips)):\n",
    "    start = i * f_clip\n",
    "    plt.gca().add_patch(plt.Rectangle((0, start), melspec_db.shape[1], f_clip, edgecolor='lime', facecolor='none', lw=2, label='Spectral Clip' if i==0 else None))\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "if handles:\n",
    "    plt.legend(handles, labels)\n",
    "plt.colorbar(label='dB')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set token dimension (embedding size)\n",
    "token_dim = 384\n",
    "\n",
    "# Define SpecTTTra-repo-based tokenizer\n",
    "class ClipTokenizer(nn.Module):\n",
    "    def __init__(self, input_dim, token_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, token_dim)\n",
    "    def forward(self, x):\n",
    "        # x: (num_clips, clip_size)\n",
    "        return self.proj(x)\n",
    "\n",
    "# Tokenize temporal clips\n",
    "temporal_clip_size = n_mels * t_clip\n",
    "temporal_tokenizer = ClipTokenizer(temporal_clip_size, token_dim)\n",
    "\n",
    "temporal_clips_flat = [torch.tensor(clip.flatten(), dtype=torch.float32) for clip in temporal_clips]\n",
    "temporal_clips_batch = torch.stack(temporal_clips_flat)  # shape: (num_temporal_clips, clip_size)\n",
    "temporal_tokens = temporal_tokenizer(temporal_clips_batch)  # shape: (num_temporal_clips, token_dim)\n",
    "\n",
    "# Tokenize spectral clips\n",
    "spectral_clip_size = f_clip * melspec_db.shape[1]\n",
    "spectral_tokenizer = ClipTokenizer(spectral_clip_size, token_dim)\n",
    "\n",
    "spectral_clips_flat = [torch.tensor(clip.flatten(), dtype=torch.float32) for clip in spectral_clips]\n",
    "spectral_clips_batch = torch.stack(spectral_clips_flat)  # shape: (num_spectral_clips, clip_size)\n",
    "spectral_tokens = spectral_tokenizer(spectral_clips_batch)  # shape: (num_spectral_clips, token_dim)\n",
    "\n",
    "# Show token shapes and a sample\n",
    "print(\"Temporal tokens shape:\", temporal_tokens.shape)\n",
    "print(\"Spectral tokens shape:\", spectral_tokens.shape)\n",
    "print(\"Sample temporal token:\", temporal_tokens[0].detach().numpy())\n",
    "print(\"Sample spectral token:\", spectral_tokens[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Parameters from config\n",
    "embed_dim = 384\n",
    "num_heads = 6\n",
    "num_layers = 12\n",
    "pe_learnable = True\n",
    "\n",
    "# Temporal_tokens and spectral_tokens are already computed and have shape (num_clips, embed_dim)\n",
    "# Concatenate both types of tokens\n",
    "all_tokens = torch.cat([temporal_tokens, spectral_tokens], dim=0)  # shape: (num_total_clips, token_dim)\n",
    "\n",
    "# Project tokens to embed_dim \n",
    "if all_tokens.shape[1] != embed_dim:\n",
    "    projector = nn.Linear(all_tokens.shape[1], embed_dim)\n",
    "    all_tokens = projector(all_tokens)\n",
    "\n",
    "num_tokens = all_tokens.shape[0]\n",
    "\n",
    "# Positional Embedding (learnable)\n",
    "if pe_learnable:\n",
    "    pos_embed = nn.Parameter(torch.zeros(1, num_tokens, embed_dim))\n",
    "    nn.init.trunc_normal_(pos_embed, std=0.02)\n",
    "    tokens_with_pos = all_tokens.unsqueeze(0) + pos_embed  # (1, num_tokens, embed_dim)\n",
    "else:\n",
    "    # Use fixed sinusoidal embedding if not learnable\n",
    "    def get_sinusoid_encoding(n_position, d_hid):\n",
    "        ''' Sinusoid position encoding table '''\n",
    "        def get_angle(pos, i):\n",
    "            return pos / (10000 ** (2 * (i // 2) / d_hid))\n",
    "        table = torch.zeros(n_position, d_hid)\n",
    "        for pos in range(n_position):\n",
    "            for i in range(d_hid):\n",
    "                table[pos, i] = get_angle(pos, i)\n",
    "        table[:, 0::2] = torch.sin(table[:, 0::2])\n",
    "        table[:, 1::2] = torch.cos(table[:, 1::2])\n",
    "        return table\n",
    "    pos_embed = get_sinusoid_encoding(num_tokens, embed_dim).unsqueeze(0)\n",
    "    tokens_with_pos = all_tokens.unsqueeze(0) + pos_embed\n",
    "\n",
    "# Transformer Encoder (PyTorch nn.TransformerEncoder)\n",
    "encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=embed_dim,\n",
    "    nhead=num_heads,\n",
    "    dim_feedforward=int(embed_dim * 2.67),  # mlp_ratio from config\n",
    "    dropout=0.1,  # pos_drop_rate, attn_drop_rate\n",
    "    activation='gelu',\n",
    "    batch_first=True\n",
    ")\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "# Forward pass through transformer\n",
    "encoded_tokens = transformer_encoder(tokens_with_pos)  # shape: (1, num_tokens, embed_dim)\n",
    "\n",
    "print(\"Encoded tokens shape:\", encoded_tokens.shape)\n",
    "print(\"Sample encoded token:\", encoded_tokens[0, 0].detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
